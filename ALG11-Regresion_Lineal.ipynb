{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8-  Machine Learning\n",
    "\n",
    "El nombre de este apartado es un poco pomposo pero es importante entender que el **√°lgebra** juega un papel fundamental en el Machine Learning y para ello vamos a ver algunos conceptos que nos van a ser √∫tiles en un futuro.\n",
    "\n",
    "Hay muchas formas de dividir los problemas de Machine Learning, algunas de ellas:<br>\n",
    "\n",
    "**<center>Supervised Learning | Unsupervised Learning</center>**\n",
    "**<center>Probabilistic Methods | Non-probabilistic Methods</center>**\n",
    "**<center>Modeling Approach | Optimization Techniques</center>**\n",
    "**<center>...</center>**\n",
    "\n",
    "Dentro de los problemas de **Supervised Learning** podemos encontrar (nos vamos a centrar solo en este caso porque vamos a explicar algunas herramientas que nos proporciona la √°lgebra para resolver ciertos problemas):\n",
    "\n",
    "<img src=\"Images/reg_clas.png\" style=\"width: 75%\"/>\n",
    "\n",
    "- **Regresi√≥n**: Dado un conjunto de datos, predecir una variable continua\n",
    "- **Clasificaci√≥n**: Dado un conjunto de datos, predecir un valor discreto (label/class...)\n",
    "\n",
    "## 8.1 - Regresi√≥n Lineal\n",
    "\n",
    "La **regresi√≥n lineal** es el caso m√°s simple de **regresi√≥n**. Imaginaros que nos hemos ido a Islandia a ver g√©iseres (Geyser) y hemos medido durante un d√≠a cada vez que hace erupci√≥n y cuanto tiempo dura la erupci√≥n. Con estos datos nos gustar√≠a predecir cuando va a ser la siguiente erupci√≥n.\n",
    "\n",
    "Si observamos los datos, vemos como en el **eje X** tenemos el tiempo que el g√©iser estuvo en erupci√≥n, y luego en el **eje y**, tenemos el tiempo que tenemos que esperar a la pr√≥xima erupci√≥n. Vemos como claramente existe una **relaci√≥n entre ambas variables, ¬øno?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/erupcion.png\" style=\"width: 340px;\" align=\"left\"/><img src=\"Images/erupcion_graph.png\" style=\"width: 290px;\" align=\"left\"/><img src=\"Images/reg_erupcion_graph.png\" style=\"width: 265px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As√≠ que mirando estos datos, **podr√≠amos querer llegar a alg√∫n tipo de funci√≥n**, una simplificaci√≥n\n",
    "donde simplemente tienes que tomar la entrada, **que es el tiempo que el g√©iser ha estado en erupci√≥n** y entonces la salida ser√° **una predicci√≥n de cu√°nto tiempo creemos que vamos a tener que esperar a la pr√≥xima erupci√≥n.** Y mirando esto, la elecci√≥n m√°s natural es modelar esto con una funci√≥n lineal. \n",
    "\n",
    "Este es el ajuste de regresi√≥n m√°s simple, donde tienen una entrada que creemos que est√° relacionada de acuerdo a alguna funci√≥n lineal con la salida. \n",
    "\n",
    "#### Funci√≥n lineal\n",
    "\n",
    "El modelo para este problema ser√° una funci√≥n lineal donde tendremos que la variable $y$ es el **tiempo de espera** y la variable $x$ es la **duraci√≥n de la erupci√≥n**.\n",
    "\n",
    "<center>$y = w_0 + x \\cdot w_1$</center><br>\n",
    "<center>$tiempo_{espera} = w_0 + duracion_{erupcion} \\cdot w_1$</center>\n",
    "\n",
    "$w_1$: *Pendiente / Slope*<br>\n",
    "$w_0$: *T√©rmino independiente / Bias / Offset / Shift...* \n",
    "\n",
    "Donde $w_0$ y $w_1$ deben ser determinados a partir de los datos de entrada.\n",
    "\n",
    "### ¬øC√≥mo trasladamos esto a problemas con un mayor n√∫mero de dimensiones?\n",
    "\n",
    "Imagina que tenemos dos entradas y queremos predecir una salida basada en esas dos entradas. En ese caso, podr√≠amos seguir utilizando una funci√≥n lineal donde:\n",
    "\n",
    "<center>$output = w_0 + input_{1} \\cdot w_1 + input_{2} \\cdot w_2$</center>\n",
    "\n",
    "Geom√©tricamente podr√≠a ser representado como:\n",
    "<img src=\"Images/map_geometric.png\" style=\"width: 340px;\"/>\n",
    "\n",
    "### Caso gen√©rico\n",
    "\n",
    "Por tanto, el objetivo de una **regresi√≥n** es:\n",
    "\n",
    "Encontrar una funci√≥n $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ tal que $y \\approx f(x;w)$ para el conjunto de datos $(x, y)$. Una regresi√≥n es denominada **lineal** si la predicci√≥n $f$ es una funci√≥n lineal con par√°metros no conocidos $w$.\n",
    "\n",
    "De una forma m√°s gen√©rica podemos escribir la **regresi√≥n** como:\n",
    "\n",
    "<center>$y_{i} \\approx  f(x_{i}; w) = w_0 + \\sum_{j=1}^d x_{ij}w_{j}$</center>\n",
    "\n",
    "*Nota:* Consideramos que tenemos m√°s observaciones que dimensiones $d<n$\n",
    "\n",
    "## 8.2 - M√≠nimos Cuadrados (Least Squares)\n",
    "\n",
    "**Genial!!** Ya sabemos que es una regresi√≥n y como se clasifican algunos problemas de ML en el caso del g√©iser, ¬øc√≥mo podemos determinar los par√°metros $w$ que nos devuelve la recta que mejor se ajusta a los datos?\n",
    "\n",
    "<img src=\"Images/interesting2.gif\" style=\"width: 340px;\"/>\n",
    "\n",
    "Si recapitulamos un poco... Tenemos un conjunto de datos $(x_1; y_1) ... (x_n, y_n)$ y queremos usar estos datos para determinar $w$, tal que $y_i \\approx f(x_i, w)$ pero antes debemos tener una **\"objective function\"** para determinar que valores de $w$ consideramos por buenos. Para ello, vamos a usar **M√≠nimos cuadrados**\n",
    "\n",
    "### M√≠nimos Cuadrados\n",
    "\n",
    "Los **m√≠nimos cuadrados** nos ayuda a elegir los valores de $w$ que minimizan la suma de los errores al cuadrado.<br><br>\n",
    "\n",
    "<center>$\\DeclareMathOperator*{\\argmin}{argmin} w_{LS} = \\argmin _w \\sum_{i=1}^n (y_{i} - f(x_{i}; w))^2$</center><br>\n",
    "\n",
    "<center>$\\DeclareMathOperator*{\\argmin}{argmin} argmin$ = Minimizar</center>\n",
    "\n",
    "Basicamente, el objetivo de los **m√≠nimos cuadrados $L$** es minimizar la suma de los errores al cuadrado. Si observamos el siguiente plano, la **distancia vertical es el error.**\n",
    "\n",
    "<img src=\"Images/least_squares.png\" style=\"width: 340px;\"/>\n",
    "\n",
    "#### Ejemplo\n",
    "\n",
    "Aqu√≠ hay un ejemplo muy sencillo donde tenemos como entrada un vector de 2 dimensiones con medidas de **educaci√≥n** y **antig√ºedad** y como salida tenemos el **salario**. Basicamente queremos predecir los ingresos de una persona en base a su educaci√≥n y antig√ºedad.\n",
    "\n",
    "<img src=\"Images/ex_least.png\" style=\"width: 340px;\" align=\"left\"/><br><br><br>\n",
    "<b>Input</b>: (Educaci√≥n, Antig√ºedad) $\\in \\mathbb{R}^2$<br><br>\n",
    "<b>Output</b>: Salario $\\in \\mathbb{R}$<br><br>\n",
    "<b>Model</b>: $salario \\approx w_0 + (educacion) \\cdot w_1 + (antig√ºedad) \\cdot w_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 - M√≠nimos Cuadrados - Matrices\n",
    "\n",
    "Existen distintas formas para la ecuaci√≥n: \n",
    "\n",
    "<center>$\\DeclareMathOperator*{\\argmin}{argmin} w_{LS} = \\argmin _w \\sum_{i=1}^n (y_{i} - f(x_{i}; w))^2$</center><br>\n",
    "\n",
    "Si desarrollamos dicha f√≥rmula podemos llegar a la ecuaci√≥n de **m√≠nimos cuadrados** en forma matricial [aqu√≠ viene el desarrollo, por si le quer√©is echar un ojo](http://pillowlab.princeton.edu/teaching/statneuro2018/slides/notes03b_LeastSquaresRegression.pdf):\n",
    "\n",
    "<center>$X^{T}X W'=X^{T}y$</center><br>\n",
    "\n",
    "O lo que es lo mismo:<br>\n",
    "<center>$W'=(X^{T}X)^{-1}X^{T}y$</center>\n",
    "    \n",
    "Donde la matrix $X$, viene definida como:\n",
    "\n",
    "<center>$\n",
    "  X =\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & x_{11} & ... & x_{1d} \\\\\n",
    "   1 & x_{21} & ... & x_{2d} \\\\\n",
    "   ... & ... & ... & ... \\\\\n",
    "   1 & x_{nd} & ... & x_{nd} \\\\\n",
    "  \\end{array} } \\right]\n",
    "$</center><br>\n",
    "\n",
    "Se le a√±ade un $1$ a cada fila de la matriz por convenci√≥n y por la componente $w_0$ de la ecuaci√≥n - Indicado por $\\mathbb{R}^{d+1}$\n",
    "\n",
    "### 8.3.1 - ¬øPor qu√© n > d?\n",
    "\n",
    "$n > d$ indica que el **n√∫mero de observaciones** sea mayor que el **el n√∫mero de dimensiones**, vamos a poner un ejemplo para entenderlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [['Teresa', 8.7, 7], ['Alex', 6.7, 8], ['Sara', 9, 9.1], ['Pepe', 7, 5.1]]\n",
    "df = pd.DataFrame(data, columns=['Name', 'Media bachiller - Grade', 'Media selectividad - Test'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El n√∫mero `n` es el n√∫mero de observaciones y el n√∫mero `d` son las dimensiones. Es decir, si en nuestro problema tenemos **3 personas** a estudiar y vamos a analizar la influencia de **media de bachiller** y de su **media en selectividad**, tendremos que:\n",
    "\n",
    "- N√∫mero observaciones `n`: 3 personas (Teresa, Alex, Sara)\n",
    "- N√∫mero de dimensiones `d`: 2 variables (Media bachiller, Media selectividad)\n",
    "\n",
    "<img src=\"./images/nd_x.jpg\" width=\"400\">\n",
    "\n",
    "En el caso de que $n < d+1$ no podemos aplicar **m√≠nimos cuadrados**, ya que $(X^TX)^{-1}$ no existe y habr√≠a un n√∫mero infinito de soluciones.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Calcula $(X^TX)^{-1}$</b><br><br>\n",
    "    \n",
    "<center>$\n",
    "  X =\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & 8.7 & 7 \\\\\n",
    "   1 & 6.7 & 8 \\\\\n",
    "   1 & 9 & 9.1 \\\\\n",
    "   1 & 7 & 5.1 \\\\\n",
    "  \\end{array} } \\right]\n",
    "$</center><br>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Calcula $(X^TX)^{-1}$</b><br><br>\n",
    "    \n",
    "<center>$\n",
    "  X =\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & 8.7 & 7 \\\\\n",
    "   1 & 6.7 & 8 \\\\\n",
    "  \\end{array} } \\right]\n",
    "$</center><br>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2 - La filosof√≠a detr√°s de los m√≠nimos cuadrados\n",
    "\n",
    "¬øOs acordais de cuando hablamos de los sistemas de ecuaciones? \n",
    "\n",
    "<img src=\"./Images/secuaciones.png\" width=50%>\n",
    "\n",
    "Cuando un sistema era incompatible significaba que ten√≠a infinitas soluciones, pero hay una forma de encontrar una soluci√≥n aproximada y aqu√≠ es donde entra en juego: **Los M√≠nimos Cuadrados**.\n",
    "\n",
    "**Supongamos que tratamos de resolver un sistema de la forma Ax = b** y que no existe soluci√≥n para este sistema. En este caso, puede ser interesante estudiar para qu√© vector o vectores $x'$, $Ax' ‚âà b$, esto es, tratamos de buscar una soluci√≥n aproximada del sistema.\n",
    "\n",
    "Una forma de interpretar esta aproximaci√≥n es la siguiente:\n",
    "Si Ax' ‚âà b, entonces podemos intentar encontrar x' tal que kAx' ‚àí bk sea lo menor posible.\n",
    "Es decir, tratamos de resolver el problema minx ‚ÄñAx ‚àí b‚Äñ \n",
    "\n",
    "Si el sistema tiene soluci√≥n, entonces el m√≠nimo del problema anterior es cero, y por tanto Ax = b; pero si no hay soluci√≥n, tiene sentido buscar el vector x que haga menor esa norma, y por tanto que m√°s se parece a una soluci√≥n.\n",
    "\n",
    "Al final, al igual que hemos hecho antes, desarrollamos dicha expresi√≥n, se llega a la conclusi√≥n de que se trata de resolver el sistema:\n",
    "\n",
    "$$A^TAx'=A^Tb$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 - \"Ampliando\" la Regresi√≥n Lineal\n",
    "\n",
    "Lo bueno de la **regresi√≥n lineal** y de los **m√≠nimos cuadrados** es que es muy vers√°til. Seguro que pens√°is que calcular una funci√≥n lineal es un problema super simple, teniendo en cuenta que solo estamos dibujando una l√≠nea a trav√©s de nuestro hiperplano, pero eso no es as√≠. Las funciones lineales son la base de **TODO**, incluso de las redes neuronales.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Os he dejado un extra sobre los perceptrones: EXTRA5_Perceptron</b>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.1 - \"Ampliando\" las regresiones lineales\n",
    "\n",
    "Bueno, volvamos a las **regresiones lineales**üòÉ. Como hemos dicho, las **regresiones lineales** y los **m√≠nimos cuadrados** son herramientas muy potentes. Recuerda que la **regresi√≥n lineal** se llama regresi√≥n lineal porque la salida es lineal:<br><br>\n",
    "\n",
    "<center>$y_{i} \\approx  f(x_{i}; w) = w_0 + \\sum_{j=1}^d x_{ij}w_{j}$</center>\n",
    "\n",
    "As√≠ que, tenemos una funci√≥n lineal de los pesos $w$, no necesariamente de las entradas $x$. En el siguiente ejemplo, la regresi√≥n lineal va a ser algo que **NO** queramos usar, porque no va a modelar correctamente nuestros datos, ¬øqu√© hacemos entonces?:\n",
    "\n",
    "<img src=\"./images/pol_regresion.png\" width=50%>\n",
    "\n",
    "Si miramos la ecuaci√≥n original de la regresi√≥n lineal, tenemos la salida $y$, que es aproximadamente igual a la entrada $x$\n",
    "multiplicado por un peso $w_1$, m√°s un desplazamiento $w_0$. B√°sicamente estamos diciendo que los datos tienen que ser modelados por una l√≠nea, pero no es lo ideal en este caso.\n",
    "\n",
    "Pero podr√≠amos decir ahora que nuestra salida $y$ sea de la forma:\n",
    "\n",
    "<img src=\"./images/pol_regresion2.png\" width=50%>\n",
    "\n",
    "Esto es lo que se denomina `regresi√≥n polinomial` usando una funci√≥n de 3¬∫ orden. Lo mejor de todo es que se resuelve de la misma forma que la `regresi√≥n lineal`, usando: \n",
    "\n",
    "<center>$X^{T}X W'=X^{T}y$</center><br>\n",
    "\n",
    "O lo que es lo mismo:<br>\n",
    "<center>$W'=(X^{T}X)^{-1}X^{T}y$</center>\n",
    "    \n",
    "La √∫nica diferencia es que nuestra matriz $X$ con un polinomio de grado $p$, ser√° de la forma:\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Nota:</b> $d=1$ indica que solo tiene una variable, por ejemplo: edad / dinero; ...\n",
    "</div><br><br>\n",
    "\n",
    "<center>$\n",
    "  X =\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & x_1 & x_1^2 & ... & x_n^p \\\\\n",
    "   1 & x_2 & x_2^2 & ... & x_n^p \\\\\n",
    "   1 & ... & ... & ... & ... \\\\\n",
    "   1 & x_n & x_n^2 & ... & x_n^p \\\\\n",
    "  \\end{array} } \\right]\n",
    "$</center>\n",
    "\n",
    "Donde `p` ser√° el orden de nuestro polinomio.\n",
    "\n",
    "#### Influencia del grado del polinomio en nuestra regresi√≥n\n",
    "\n",
    "<img src=\"./images/gif-pol-reg.gif\" width=50%>\n",
    "\n",
    "### - Regresi√≥n polinomial en $\\mathbb{R}^2$ (2 dimensiones)\n",
    "\n",
    "Como hemos visto la regresi√≥n polinomial se resuelve de la misma forma que una regresi√≥n linea, la √∫nica diferencia es que tendremos m√°s componentes en nuestra matriz $X$. El `n√∫mero de columnas` vendr√° dado por:\n",
    "\n",
    "<center>$(orden \\,\\, polinomio)\\, x \\,(dimensiones) + 1$</center><br>\n",
    "\n",
    "En ese caso para un **polinomio de orden 2**:<br>\n",
    "<center>$\n",
    "  y_i = w_0 + w_1x_{i1} + w_2x_{i2} + w_3x_{i1}^2 + w_4x_{i2}^2\n",
    "$</center>\n",
    "\n",
    "Para un **polinomio de orden 3**:<br>\n",
    "\n",
    "<center>$\n",
    "  y_i = w_0 + w_1x_{i1} + w_2x_{i2} + w_3x_{i1}^2 + w_4x_{i2}^2 + w_5x_{i1}^3 + w_6x_{i2}^3\n",
    "$</center>\n",
    "\n",
    "<img src=\"./images/pol_3order.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.2 - Resumen\n",
    "\n",
    "#### ¬øQu√© es la regresi√≥n?\n",
    "\n",
    "La `regresi√≥n` es un modelo matem√°tico que busca determinar la relaci√≥n entre una variable dependiente $y$ con respecto a otras variables independientes $x_n$.\n",
    "\n",
    "En la regresi√≥n, dado un conjunto de datos, nos va a predecir una variable continua. \n",
    "\n",
    "<img src=\"./images/regresion.jpg\" width=50%>\n",
    "\n",
    "#### ¬øCu√°l es el objetivo de la regresi√≥n?\n",
    "\n",
    "El objetivo de una **regresi√≥n** es encontrar una funci√≥n $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ tal que $y \\approx f(x;w)$ para el conjunto de datos $(x, y)$. Una regresi√≥n es denominada **lineal** si la predicci√≥n $f$ es una funci√≥n lineal con par√°metros no conocidos $w$.\n",
    "\n",
    "De una forma m√°s gen√©rica podemos escribir la **regresi√≥n** como:\n",
    "\n",
    "<center>$y_{i} \\approx  f(x_{i}; w) = w_0 + \\sum_{j=1}^d x_{ij}w_{j}$</center>\n",
    "\n",
    "*Nota:* Consideramos que tenemos m√°s observaciones que dimensiones $n > d$<br><br>\n",
    "<center>$\n",
    "  X =\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & x_{11} & ... & x_{1d} \\\\\n",
    "   1 & x_{21} & ... & x_{2d} \\\\\n",
    "   ... & ... & ... & ... \\\\\n",
    "   1 & x_{nd} & ... & x_{nd} \\\\\n",
    "  \\end{array} } \\right]\n",
    "$</center>\n",
    "\n",
    "#### ¬øC√≥mo la resolvemos?\n",
    "\n",
    "Usando **m√≠nimos cuadrados** cuyo objetivo es encontrar los valores de $w$ que minimizan la suma de los errores al cuadrado.\n",
    "Existen distintas formas para la ecuaci√≥n: \n",
    "\n",
    "<center>$\\DeclareMathOperator*{\\argmin}{argmin} w_{LS} = \\argmin _w \\sum_{i=1}^n (y_{i} - f(x_{i}; w))^2$</center><br>\n",
    "\n",
    "Si desarrollamos dicha f√≥rmula podemos llegar a la ecuaci√≥n de **m√≠nimos cuadrados** en forma matricial [aqu√≠ viene el desarrollo, por si le quer√©is echar un ojo](http://pillowlab.princeton.edu/teaching/statneuro2018/slides/notes03b_LeastSquaresRegression.pdf):\n",
    "\n",
    "<center>$X^{T}X W'=X^{T}y$</center><br>\n",
    "\n",
    "O lo que es lo mismo:<br>\n",
    "<center>$W'=(X^{T}X)^{-1}X^{T}y$</center>\n",
    "    \n",
    "    \n",
    "#### Regresion polinomial\n",
    "\n",
    "Para casos en los que no podamos modelar nuestro problema con una **funci√≥n lineal** podemos aplicar un polinomio para ello. En este caso se llamar√≠a **regresi√≥n polinomial** y se resuelve de la misma forma:\n",
    "\n",
    "<center>$X^{T}X W'=X^{T}y$</center><br>\n",
    "<center>$W'=(X^{T}X)^{-1}X^{T}y$</center>\n",
    "\n",
    "#### Matriz X\n",
    "\n",
    "La √∫nica diferencia entre la regresi√≥n lineal y la regresi√≥n polinomial es como escribimos la matriz $X$, aqu√≠ algunos ejemplos:\n",
    "\n",
    "- Problema con 2 dimensiones\n",
    "\n",
    "<center>$\n",
    "  X =\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & x_{1d} & x_{1d} & ... & x_{nd} \\\\\n",
    "   1 & x_{2d} & x_{2d} & ... & x_{nd} \\\\\n",
    "   1 & ... & ... & ... & ... \\\\\n",
    "   1 & x_{nd} & x_{nd} & ... & x_{nd} \\\\\n",
    "  \\end{array} } \\right] = \\left[ {\\begin{array}{cc}\n",
    "   1 & x_{11} & x_{12}\\\\\n",
    "   1 & x_{21} & x_{22}\\\\\n",
    "   1 & ... & ...\\\\\n",
    "   1 & x_{n1} & x_{n2}\\\\\n",
    "  \\end{array} } \\right] \n",
    "$</center>\n",
    "\n",
    "- Problema con 3 dimensiones y 4 observaciones\n",
    "\n",
    "<center>$\n",
    "  X =\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & x_{1d} & x_{1d} & x_{1d} &... & x_{nd} \\\\\n",
    "   1 & x_{2d} & x_{2d} & x_{2d} &... & x_{nd} \\\\\n",
    "   1 & x_{3d} & x_{3d} & x_{3d} &... & x_{nd} \\\\\n",
    "   1 & x_{4d} & x_{4d} & x_{4d} &... & x_{nd} \\\\\n",
    "  \\end{array} } \\right] = \\left[ {\\begin{array}{cc}\n",
    "   1 & x_{11} & x_{12} & x_{13} \\\\\n",
    "   1 & x_{21} & x_{22} & x_{23} \\\\\n",
    "   1 & x_{31} & x_{32} & x_{33} \\\\\n",
    "   1 & x_{41} & x_{42} & x_{43} \\\\\n",
    "  \\end{array} } \\right] \n",
    "$</center>\n",
    "\n",
    "- Problema con 4 observaciones y 2 dimensiones, modelado con un polinomio de 2¬∫ grado\n",
    "\n",
    "<center>$\n",
    "  X =\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & x_{1d} & x_{1d} & x_{1d}^2 &... & x_{nd}^p \\\\\n",
    "   1 & x_{2d} & x_{2d} & x_{2d}^2 &... & x_{nd}^p \\\\\n",
    "   1 & x_{3d} & x_{3d} & x_{3d}^2 &... & x_{nd}^p \\\\\n",
    "   1 & x_{4d} & x_{4d} & x_{4d}^2 &... & x_{nd}^p \\\\\n",
    "  \\end{array} } \\right] = \\left[ {\\begin{array}{cc}\n",
    "   1 & x_{11} & x_{12} & x_{11}^2 & x_{12}^2 \\\\\n",
    "   1 & x_{21} & x_{22} & x_{21}^2 & x_{22}^2 \\\\\n",
    "   1 & x_{31} & x_{32} & x_{31}^2 & x_{32}^2 \\\\\n",
    "   1 & x_{41} & x_{42} & x_{41}^2 & x_{42}^2 \\\\\n",
    "  \\end{array} } \\right] \n",
    "$</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Gradient Descent\n",
    "\n",
    "Hemos visto como **Least Squares/M√≠nimos Cuadrados** nos permite encontrar la recta que mejor aproxima nuestros datos. Los m√©todos que nos permiten encontrar la soluci√≥n m√≠nima o m√°xima se llaman **m√©todos de optimizaci√≥n**. Existe numerosos tipos de m√©todos de optimizaci√≥n, pero nos vamos a centrar en uno de los m√°s usados hoy en d√≠a en el mundo del Machine Learning - **Gradient Descent**.\n",
    "\n",
    "**Gradient Descent** es un m√©todo de optimizaci√≥n iterativo (cada iteraci√≥n que ejecutamos estamos un poco m√°s cerca del m√≠nimo de la funci√≥n - soluci√≥n). \n",
    " \n",
    "\n",
    "<center>$y_{i} \\approx  f(x_{i}; w) = w_0 + \\sum_{j=1}^d x_{ij}w_{j}$</center>\n",
    "\n",
    "Podemos convertirlo a matrices:\n",
    "\n",
    "<center>$y_{i} \\approx  f(x_{i}; w) = XW$</center>\n",
    "\n",
    "Y queremos encontrar:\n",
    "\n",
    "* **Los argumentos $w$ que minimiza la funci√≥n** - $\\underset{w}{\\operatorname{argmin}}f(w)$\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Nota</b> $\\min f(x)$ y $\\arg \\min f(x)$ son cosas distintas. Un ejemplo:\n",
    "</div> \n",
    "\n",
    "Considera la funci√≥n $f(x) = 100 + (x - 3)^2$\n",
    "\n",
    "$\\min f(x) = f(\\arg \\min f(x))$\n",
    "\n",
    "Entonces:\n",
    "\n",
    "$\\frac{df(x)}{x} = 2(x - 3)$<br>\n",
    "$\\arg \\min 2(x - 3) = 0$<br>\n",
    "$\\arg \\min f(x) = 3$\n",
    "\n",
    "Por otro lado:\n",
    "\n",
    "$\\min f(x) = f(\\arg \\min f(x)) = 100 + (3 - 3)^2$<br>\n",
    "$\\min f(x) = 100$\n",
    "\n",
    "Podemos graficarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axs = plt.subplots() \n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "# calculate the y value for each element of the x vector\n",
    "y = 100 + (x - 3)**2 \n",
    "\n",
    "axs.annotate('arg min f(x)',\n",
    "            xy=(0.85, 0), xycoords='axes fraction',\n",
    "            xytext=(-0, 0), textcoords='offset pixels',\n",
    "            horizontalalignment='right',\n",
    "            verticalalignment='bottom',\n",
    "            color=\"r\")\n",
    "\n",
    "axs.annotate('min f(x)',\n",
    "            xy=(0, 0.1), xycoords='axes fraction',\n",
    "            xytext=(-0, 0), textcoords='offset pixels',\n",
    "            horizontalalignment='right',\n",
    "            verticalalignment='bottom',\n",
    "            color=\"r\")\n",
    "\n",
    "axs.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como queremos encontrar el valor de $w$ que hace m√≠nimo $f(x)$, es decir $\\arg \\min f(x)$, los pasos a seguir son:\n",
    "\n",
    "* Empezar en alg√∫n punto aleatorio de la curva\n",
    "* Iterativamente, movernos en la direcci√≥n de mayor pendiente $\\nabla f(x)$ con pasos con tama√±o $\\eta$\n",
    "\n",
    "\\begin{equation}\n",
    "    x := x - \\eta \\cdot \\nabla f(x)\n",
    "\\end{equation}\n",
    "\n",
    "El par√°metro $\\eta$ se denomina **learning rate** y es el par√°metro que se encarga de medir el \"step size\" en cada iteraci√≥n mientras nos movemos hacia el m√≠nimo de la funci√≥n. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>¬øPor qu√© el valor negativo en la ecuaci√≥n?</b>\n",
    "</div> \n",
    "\n",
    "\n",
    "**DEMO GRADIENT DESCENT:** https://developers.google.com/machine-learning/crash-course/fitter/graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GD(): \n",
    "    def __init__(self, eta=0.1, x0=0, max_iter=50, diff_to_stop=0.01, delta=0.01): \n",
    "        self.x0 = x0 #randomly initialize any value\n",
    "        self.delta = delta #used for gradient calculations\n",
    "        self.eta = eta #learning rate \n",
    "        self.diff_to_stop = diff_to_stop #stop the algorithms if steps are smaller than this value\n",
    "        self.max_iter = max_iter #when to stop\n",
    "\n",
    "    def run(self, f, plot_method=None):       \n",
    "        plt.figure(figsize=(8, 4))\n",
    "        xx = np.linspace(-10, 10, 100)\n",
    "        yy = f(xx)\n",
    "        plt.plot(xx, yy)\n",
    "        plt.grid()\n",
    "        \n",
    "        x_new = self.x0\n",
    "        iter_no = 0\n",
    "        while True:\n",
    "            iter_no += 1\n",
    "\n",
    "            #main algorithm\n",
    "            x = x_new #note: x_new is the position of the previous move\n",
    "            grad = (f(x+self.delta) - f(x))/self.delta #evaluate the gradient at f(x)\n",
    "            x_new = x - self.eta*grad #move in the direction of gradient\n",
    "            #end of main algorithm\n",
    "\n",
    "            if plot_method is 'scatter':\n",
    "                plt.scatter(x_new, f(x_new), color='r')\n",
    "            else:\n",
    "                plt.arrow(x, f(x), x_new-x, f(x_new)-f(x), head_width=0.3, head_length=2, color='r')\n",
    "\n",
    "            step_size = np.abs(x_new - x)\n",
    "            if step_size <= self.diff_to_stop:\n",
    "                print('Successfully converged with a step size of {} after {} iterations!'.format(step_size, iter_no))\n",
    "                plt.scatter(x_new, f(x_new), s=200, marker='*')\n",
    "                break\n",
    "            elif iter_no == self.max_iter:\n",
    "                print('Max iterations completed! Convergence cannot be guaranteed. Step size={}'.format(step_size))\n",
    "                plt.scatter(x_new, f(x_new), s=200, marker='*')\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Vamos a analizar que sucede cuando modificamos los par√°metros</b>\n",
    "</div> \n",
    "\n",
    "* Incrementa el n√∫mero de iteraciones a 50 Increase the maximum number of iterations in order to 50.\n",
    "* Cambia el learning rate a diferentes valores $\\eta \\in \\{0.05, 0.1, 0.8, 1.1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, p=1, q=0, r=0):\n",
    "    return p*(x-q)**2 + r\n",
    "\n",
    "GD(eta=0.00001, x0=8, max_iter=5, diff_to_stop=0.01, delta=0.01).run(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øQu√© sucede en este caso? \n",
    "$$f(x) = -\\exp(-(x-2)^2) - 0.5 \\exp(-(x+2)^2)$$\n",
    "\n",
    "Sugerencia: max_iter en $100$, x0 en $-3$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_multimodal(x):\n",
    "    return -np.exp(-(x-2)**2) - 0.5*np.exp(-(x+2)**2)\n",
    "\n",
    "GD(eta=0.1, x0=-3, max_iter=100, diff_to_stop=0.01, delta=0.01).run(f_multimodal, plot_method='scatter')\n",
    "\n",
    "# Local minimum - Alguna soluci√≥n?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent para Linear Regression\n",
    "\n",
    "Aplicar Gradient Descent para un modelo de regression lineal no tiene ninguna complejidad. Nuestro modelo viene dado por la siguiente ecuaci√≥n:\n",
    "\n",
    "$$f(x)=w_0+w^T x.$$\n",
    "\n",
    "Debemos optimizar esa funci√≥n para encontrar los valores de $w$ que la hacen m√≠nima, ¬øc√≥mo medimos \"qu√© la hace m√≠nima y qu√© no\"? Usando **Residual Sum Squares (RSS)**, que basicamente mide cuanto se aleja el modelo de los valores reales.\n",
    "\n",
    "<img src=\"Images/least_squares.png\" style=\"width: 340px;\"/>\n",
    "\n",
    "**¬øPero esto no es lo mismo que Least Squares?** Si y no... **Least Squares es un m√©todo para construir un modelo y RSS es una medida**. Se basan en lo mismo, en medir la distancia entre los valores predichos y los valores verdaderos. Entonces, aplicando RSS a nuestra regresi√≥n:\n",
    "\n",
    "$$ RSS(w) = \\sum_{n=1}^{N}[y_n-f(x_n)]^2 =  \\sum_{n=1}^{N}[y_n- (w_0 + \\sum_{d=1}^{D}w_dx_{nd}) ]^2 .$$\n",
    "\n",
    "$$ RSS(w) = \\frac{1}{2}\\sum_{n=1}^{N}[y_n-f(x_n)]^2$$\n",
    "\n",
    "Y lo que queremos es minimizar esta distancia, para que el modelo se acerque lo m√°ximo posible a los valores verdaderos.\n",
    "\n",
    "$$\\nabla RSS(w) = X^T(Xw^t-y)$$\n",
    "\n",
    "En resumen, el gradient descendiente para una regresi√≥n lineal, se basa en resolver esta ecuaci√≥n de forma iterativa:\n",
    "\n",
    "$$w^{t+1} = w^t - \\eta * \\nabla RSS(w)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
