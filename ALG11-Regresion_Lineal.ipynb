{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8-  Machine Learning\n",
    "\n",
    "El nombre de este apartado es un poco pomposo pero es importante entender que el **álgebra** juega un papel fundamental en el Machine Learning y para ello vamos a ver algunos conceptos que nos van a ser útiles en un futuro.\n",
    "\n",
    "Hay muchas formas de dividir los problemas de Machine Learning, algunas de ellas:<br>\n",
    "\n",
    "**<center>Supervised Learning | Unsupervised Learning</center>**\n",
    "**<center>Probabilistic Methods | Non-probabilistic Methods</center>**\n",
    "**<center>Modeling Approach | Optimization Techniques</center>**\n",
    "**<center>...</center>**\n",
    "\n",
    "Dentro de los problemas de **Supervised Learning** podemos encontrar (nos vamos a centrar solo en este caso porque vamos a explicar algunas herramientas que nos proporciona la álgebra para resolver ciertos problemas):\n",
    "\n",
    "<img src=\"Images/reg_clas.png\" style=\"width: 75%\"/>\n",
    "\n",
    "- **Regresión**: Dado un conjunto de datos, predecir una variable continua\n",
    "- **Clasificación**: Dado un conjunto de datos, predecir un valor discreto (label/class...)\n",
    "\n",
    "## 8.1 - Regresión Lineal\n",
    "\n",
    "La **regresión lineal** es el caso más simple de **regresión**. Imaginaros que nos hemos ido a Islandia a ver géiseres (Geyser) y hemos medido durante un día cada vez que hace erupción y cuanto tiempo dura la erupción. Con estos datos nos gustaría predecir cuando va a ser la siguiente erupción.\n",
    "\n",
    "Si observamos los datos, vemos como en el **eje X** tenemos el tiempo que el géiser estuvo en erupción, y luego en el **eje y**, tenemos el tiempo que tenemos que esperar a la próxima erupción. Vemos como claramente existe una **relación entre ambas variables, ¿no?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/erupcion.png\" style=\"width: 340px;\" align=\"left\"/><img src=\"Images/erupcion_graph.png\" style=\"width: 290px;\" align=\"left\"/><img src=\"Images/reg_erupcion_graph.png\" style=\"width: 265px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así que mirando estos datos, **podríamos querer llegar a algún tipo de función**, una simplificación\n",
    "donde simplemente tienes que tomar la entrada, **que es el tiempo que el géiser ha estado en erupción** y entonces la salida será **una predicción de cuánto tiempo creemos que vamos a tener que esperar a la próxima erupción.** Y mirando esto, la elección más natural es modelar esto con una función lineal. \n",
    "\n",
    "Este es el ajuste de regresión más simple, donde tienen una entrada que creemos que está relacionada de acuerdo a alguna función lineal con la salida. \n",
    "\n",
    "#### Función lineal\n",
    "\n",
    "El modelo para este problema será una función lineal donde tendremos que la variable $y$ es el **tiempo de espera** y la variable $x$ es la **duración de la erupción**.\n",
    "\n",
    "<center>$y = w_0 + x \\cdot w_1$</center><br>\n",
    "<center>$tiempo_{espera} = w_0 + duracion_{erupcion} \\cdot w_1$</center>\n",
    "\n",
    "$w_1$: *Pendiente / Slope*<br>\n",
    "$w_0$: *Término independiente / Bias / Offset / Shift...* \n",
    "\n",
    "Donde $w_0$ y $w_1$ deben ser determinados a partir de los datos de entrada.\n",
    "\n",
    "### ¿Cómo trasladamos esto a problemas con un mayor número de dimensiones?\n",
    "\n",
    "Imagina que tenemos dos entradas y queremos predecir una salida basada en esas dos entradas. En ese caso, podríamos seguir utilizando una función lineal donde:\n",
    "\n",
    "<center>$output = w_0 + input_{1} \\cdot w_1 + input_{2} \\cdot w_2$</center>\n",
    "\n",
    "Geométricamente podría ser representado como:\n",
    "<img src=\"Images/map_geometric.png\" style=\"width: 340px;\"/>\n",
    "\n",
    "### Caso genérico\n",
    "\n",
    "Por tanto, el objetivo de una **regresión** es:\n",
    "\n",
    "Encontrar una función $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ tal que $y \\approx f(x;w)$ para el conjunto de datos $(x, y)$. Una regresión es denominada **lineal** si la predicción $f$ es una función lineal con parámetros no conocidos $w$.\n",
    "\n",
    "De una forma más genérica podemos escribir la **regresión** como:\n",
    "\n",
    "<center>$y_{i} \\approx  f(x_{i}; w) = w_0 + \\sum_{j=1}^d x_{ij}w_{j}$</center>\n",
    "\n",
    "*Nota:* Consideramos que tenemos más observaciones que dimensiones $d<n$\n",
    "\n",
    "## 8.2 - Mínimos Cuadrados (Least Squares)\n",
    "\n",
    "**Genial!!** Ya sabemos que es una regresión y como se clasifican algunos problemas de ML en el caso del géiser, ¿cómo podemos determinar los parámetros $w$ que nos devuelve la recta que mejor se ajusta a los datos?\n",
    "\n",
    "<img src=\"Images/interesting2.gif\" style=\"width: 340px;\"/>\n",
    "\n",
    "Si recapitulamos un poco... Tenemos un conjunto de datos $(x_1; y_1) ... (x_n, y_n)$ y queremos usar estos datos para determinar $w$, tal que $y_i \\approx f(x_i, w)$ pero antes debemos tener una **\"objective function\"** para determinar que valores de $w$ consideramos por buenos. Para ello, vamos a usar **Mínimos cuadrados**\n",
    "\n",
    "### Mínimos Cuadrados\n",
    "\n",
    "Los **mínimos cuadrados** nos ayuda a elegir los valores de $w$ que minimizan la suma de los errores al cuadrado.<br><br>\n",
    "\n",
    "<center>$\\DeclareMathOperator*{\\argmin}{argmin} w_{LS} = \\argmin _w \\sum_{i=1}^n (y_{i} - f(x_{i}; w))^2$</center><br>\n",
    "\n",
    "<center>$\\DeclareMathOperator*{\\argmin}{argmin} argmin$ = Minimizar</center>\n",
    "\n",
    "Basicamente, el objetivo de los **mínimos cuadrados $L$** es minimizar la suma de los errores al cuadrado. Si observamos el siguiente plano, la **distancia vertical es el error.**\n",
    "\n",
    "<img src=\"Images/least_squares.png\" style=\"width: 340px;\"/>\n",
    "\n",
    "#### Ejemplo\n",
    "\n",
    "Aquí hay un ejemplo muy sencillo donde tenemos como entrada un vector de 2 dimensiones con medidas de **educación** y **antigüedad** y como salida tenemos el **salario**. Basicamente queremos predecir los ingresos de una persona en base a su educación y antigüedad.\n",
    "\n",
    "<img src=\"Images/ex_least.png\" style=\"width: 340px;\" align=\"left\"/><br><br><br>\n",
    "<b>Input</b>: (Educación, Antigüedad) $\\in \\mathbb{R}^2$<br><br>\n",
    "<b>Output</b>: Salario $\\in \\mathbb{R}$<br><br>\n",
    "<b>Model</b>: $salario \\approx w_0 + (educacion) \\cdot w_1 + (antigüedad) \\cdot w_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 - Mínimos Cuadrados - Matrices\n",
    "\n",
    "Existen distintas formas para la ecuación: \n",
    "\n",
    "<center>$\\DeclareMathOperator*{\\argmin}{argmin} w_{LS} = \\argmin _w \\sum_{i=1}^n (y_{i} - f(x_{i}; w))^2$</center><br>\n",
    "\n",
    "Si desarrollamos dicha fórmula podemos llegar a la ecuación de **mínimos cuadrados** en forma matricial [aquí viene el desarrollo, por si le queréis echar un ojo](http://pillowlab.princeton.edu/teaching/statneuro2018/slides/notes03b_LeastSquaresRegression.pdf):\n",
    "\n",
    "<center>$X^{T}X W'=X^{T}y$</center><br>\n",
    "\n",
    "O lo que es lo mismo:<br>\n",
    "<center>$W'=(X^{T}X)^{-1}X^{T}y$</center>\n",
    "    \n",
    "Donde la matrix $X$, viene definida como:\n",
    "\n",
    "<center>$\n",
    "  X =\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & x_{11} & ... & x_{1d} \\\\\n",
    "   1 & x_{21} & ... & x_{2d} \\\\\n",
    "   ... & ... & ... & ... \\\\\n",
    "   1 & x_{nd} & ... & x_{nd} \\\\\n",
    "  \\end{array} } \\right]\n",
    "$</center><br>\n",
    "\n",
    "Se le añade un $1$ a cada fila de la matriz por convención y por la componente $w_0$ de la ecuación - Indicado por $\\mathbb{R}^{d+1}$\n",
    "\n",
    "### 8.3.1 - ¿Por qué n > d?\n",
    "\n",
    "$n > d$ indica que el **número de observaciones** sea mayor que el **el número de dimensiones**, vamos a poner un ejemplo para entenderlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [['Teresa', 8.7, 7], ['Alex', 6.7, 8], ['Sara', 9, 9.1], ['Pepe', 7, 5.1]]\n",
    "df = pd.DataFrame(data, columns=['Name', 'Media bachiller - Grade', 'Media selectividad - Test'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El número `n` es el número de observaciones y el número `d` son las dimensiones. Es decir, si en nuestro problema tenemos **3 personas** a estudiar y vamos a analizar la influencia de **media de bachiller** y de su **media en selectividad**, tendremos que:\n",
    "\n",
    "- Número observaciones `n`: 3 personas (Teresa, Alex, Sara)\n",
    "- Número de dimensiones `d`: 2 variables (Media bachiller, Media selectividad)\n",
    "\n",
    "<img src=\"./images/nd_x.jpg\" width=\"400\">\n",
    "\n",
    "En el caso de que $n < d+1$ no podemos aplicar **mínimos cuadrados**, ya que $(X^TX)^{-1}$ no existe y habría un número infinito de soluciones.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Calcula $(X^TX)^{-1}$</b><br><br>\n",
    "    \n",
    "<center>$\n",
    "  X =\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & 8.7 & 7 \\\\\n",
    "   1 & 6.7 & 8 \\\\\n",
    "   1 & 9 & 9.1 \\\\\n",
    "   1 & 7 & 5.1 \\\\\n",
    "  \\end{array} } \\right]\n",
    "$</center><br>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Calcula $(X^TX)^{-1}$</b><br><br>\n",
    "    \n",
    "<center>$\n",
    "  X =\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & 8.7 & 7 \\\\\n",
    "   1 & 6.7 & 8 \\\\\n",
    "  \\end{array} } \\right]\n",
    "$</center><br>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2 - La filosofía detrás de los mínimos cuadrados\n",
    "\n",
    "¿Os acordais de cuando hablamos de los sistemas de ecuaciones? \n",
    "\n",
    "<img src=\"./Images/secuaciones.png\" width=50%>\n",
    "\n",
    "Cuando un sistema era incompatible significaba que tenía infinitas soluciones, pero hay una forma de encontrar una solución aproximada y aquí es donde entra en juego: **Los Mínimos Cuadrados**.\n",
    "\n",
    "**Supongamos que tratamos de resolver un sistema de la forma Ax = b** y que no existe solución para este sistema. En este caso, puede ser interesante estudiar para qué vector o vectores $x'$, $Ax' ≈ b$, esto es, tratamos de buscar una solución aproximada del sistema.\n",
    "\n",
    "Una forma de interpretar esta aproximación es la siguiente:\n",
    "Si Ax' ≈ b, entonces podemos intentar encontrar x' tal que kAx' − bk sea lo menor posible.\n",
    "Es decir, tratamos de resolver el problema minx ‖Ax − b‖ \n",
    "\n",
    "Si el sistema tiene solución, entonces el mínimo del problema anterior es cero, y por tanto Ax = b; pero si no hay solución, tiene sentido buscar el vector x que haga menor esa norma, y por tanto que más se parece a una solución.\n",
    "\n",
    "Al final, al igual que hemos hecho antes, desarrollamos dicha expresión, se llega a la conclusión de que se trata de resolver el sistema:\n",
    "\n",
    "$$A^TAx'=A^Tb$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 - \"Ampliando\" la Regresión Lineal\n",
    "\n",
    "Lo bueno de la **regresión lineal** y de los **mínimos cuadrados** es que es muy versátil. Seguro que pensáis que calcular una función lineal es un problema super simple, teniendo en cuenta que solo estamos dibujando una línea a través de nuestro hiperplano, pero eso no es así. Las funciones lineales son la base de **TODO**, incluso de las redes neuronales.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Os he dejado un extra sobre los perceptrones: EXTRA5_Perceptron</b>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.1 - \"Ampliando\" las regresiones lineales\n",
    "\n",
    "Bueno, volvamos a las **regresiones lineales**😃. Como hemos dicho, las **regresiones lineales** y los **mínimos cuadrados** son herramientas muy potentes. Recuerda que la **regresión lineal** se llama regresión lineal porque la salida es lineal:<br><br>\n",
    "\n",
    "<center>$y_{i} \\approx  f(x_{i}; w) = w_0 + \\sum_{j=1}^d x_{ij}w_{j}$</center>\n",
    "\n",
    "Así que, tenemos una función lineal de los pesos $w$, no necesariamente de las entradas $x$. En el siguiente ejemplo, la regresión lineal va a ser algo que **NO** queramos usar, porque no va a modelar correctamente nuestros datos, ¿qué hacemos entonces?:\n",
    "\n",
    "<img src=\"./images/pol_regresion.png\" width=50%>\n",
    "\n",
    "Si miramos la ecuación original de la regresión lineal, tenemos la salida $y$, que es aproximadamente igual a la entrada $x$\n",
    "multiplicado por un peso $w_1$, más un desplazamiento $w_0$. Básicamente estamos diciendo que los datos tienen que ser modelados por una línea, pero no es lo ideal en este caso.\n",
    "\n",
    "Pero podríamos decir ahora que nuestra salida $y$ sea de la forma:\n",
    "\n",
    "<img src=\"./images/pol_regresion2.png\" width=50%>\n",
    "\n",
    "Esto es lo que se denomina `regresión polinomial` usando una función de 3º orden. Lo mejor de todo es que se resuelve de la misma forma que la `regresión lineal`, usando: \n",
    "\n",
    "<center>$X^{T}X W'=X^{T}y$</center><br>\n",
    "\n",
    "O lo que es lo mismo:<br>\n",
    "<center>$W'=(X^{T}X)^{-1}X^{T}y$</center>\n",
    "    \n",
    "La única diferencia es que nuestra matriz $X$ con un polinomio de grado $p$, será de la forma:\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Nota:</b> $d=1$ indica que solo tiene una variable, por ejemplo: edad / dinero; ...\n",
    "</div><br><br>\n",
    "\n",
    "<center>$\n",
    "  X =\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & x_1 & x_1^2 & ... & x_n^p \\\\\n",
    "   1 & x_2 & x_2^2 & ... & x_n^p \\\\\n",
    "   1 & ... & ... & ... & ... \\\\\n",
    "   1 & x_n & x_n^2 & ... & x_n^p \\\\\n",
    "  \\end{array} } \\right]\n",
    "$</center>\n",
    "\n",
    "Donde `p` será el orden de nuestro polinomio.\n",
    "\n",
    "#### Influencia del grado del polinomio en nuestra regresión\n",
    "\n",
    "<img src=\"./images/gif-pol-reg.gif\" width=50%>\n",
    "\n",
    "### - Regresión polinomial en $\\mathbb{R}^2$ (2 dimensiones)\n",
    "\n",
    "Como hemos visto la regresión polinomial se resuelve de la misma forma que una regresión linea, la única diferencia es que tendremos más componentes en nuestra matriz $X$. El `número de columnas` vendrá dado por:\n",
    "\n",
    "<center>$(orden \\,\\, polinomio)\\, x \\,(dimensiones) + 1$</center><br>\n",
    "\n",
    "En ese caso para un **polinomio de orden 2**:<br>\n",
    "<center>$\n",
    "  y_i = w_0 + w_1x_{i1} + w_2x_{i2} + w_3x_{i1}^2 + w_4x_{i2}^2\n",
    "$</center>\n",
    "\n",
    "Para un **polinomio de orden 3**:<br>\n",
    "\n",
    "<center>$\n",
    "  y_i = w_0 + w_1x_{i1} + w_2x_{i2} + w_3x_{i1}^2 + w_4x_{i2}^2 + w_5x_{i1}^3 + w_6x_{i2}^3\n",
    "$</center>\n",
    "\n",
    "<img src=\"./images/pol_3order.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.2 - Resumen\n",
    "\n",
    "#### ¿Qué es la regresión?\n",
    "\n",
    "La `regresión` es un modelo matemático que busca determinar la relación entre una variable dependiente $y$ con respecto a otras variables independientes $x_n$.\n",
    "\n",
    "En la regresión, dado un conjunto de datos, nos va a predecir una variable continua. \n",
    "\n",
    "<img src=\"./images/regresion.jpg\" width=50%>\n",
    "\n",
    "#### ¿Cuál es el objetivo de la regresión?\n",
    "\n",
    "El objetivo de una **regresión** es encontrar una función $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ tal que $y \\approx f(x;w)$ para el conjunto de datos $(x, y)$. Una regresión es denominada **lineal** si la predicción $f$ es una función lineal con parámetros no conocidos $w$.\n",
    "\n",
    "De una forma más genérica podemos escribir la **regresión** como:\n",
    "\n",
    "<center>$y_{i} \\approx  f(x_{i}; w) = w_0 + \\sum_{j=1}^d x_{ij}w_{j}$</center>\n",
    "\n",
    "*Nota:* Consideramos que tenemos más observaciones que dimensiones $n > d$<br><br>\n",
    "<center>$\n",
    "  X =\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & x_{11} & ... & x_{1d} \\\\\n",
    "   1 & x_{21} & ... & x_{2d} \\\\\n",
    "   ... & ... & ... & ... \\\\\n",
    "   1 & x_{nd} & ... & x_{nd} \\\\\n",
    "  \\end{array} } \\right]\n",
    "$</center>\n",
    "\n",
    "#### ¿Cómo la resolvemos?\n",
    "\n",
    "Usando **mínimos cuadrados** cuyo objetivo es encontrar los valores de $w$ que minimizan la suma de los errores al cuadrado.\n",
    "Existen distintas formas para la ecuación: \n",
    "\n",
    "<center>$\\DeclareMathOperator*{\\argmin}{argmin} w_{LS} = \\argmin _w \\sum_{i=1}^n (y_{i} - f(x_{i}; w))^2$</center><br>\n",
    "\n",
    "Si desarrollamos dicha fórmula podemos llegar a la ecuación de **mínimos cuadrados** en forma matricial [aquí viene el desarrollo, por si le queréis echar un ojo](http://pillowlab.princeton.edu/teaching/statneuro2018/slides/notes03b_LeastSquaresRegression.pdf):\n",
    "\n",
    "<center>$X^{T}X W'=X^{T}y$</center><br>\n",
    "\n",
    "O lo que es lo mismo:<br>\n",
    "<center>$W'=(X^{T}X)^{-1}X^{T}y$</center>\n",
    "    \n",
    "    \n",
    "#### Regresion polinomial\n",
    "\n",
    "Para casos en los que no podamos modelar nuestro problema con una **función lineal** podemos aplicar un polinomio para ello. En este caso se llamaría **regresión polinomial** y se resuelve de la misma forma:\n",
    "\n",
    "<center>$X^{T}X W'=X^{T}y$</center><br>\n",
    "<center>$W'=(X^{T}X)^{-1}X^{T}y$</center>\n",
    "\n",
    "#### Matriz X\n",
    "\n",
    "La única diferencia entre la regresión lineal y la regresión polinomial es como escribimos la matriz $X$, aquí algunos ejemplos:\n",
    "\n",
    "- Problema con 2 dimensiones\n",
    "\n",
    "<center>$\n",
    "  X =\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & x_{1d} & x_{1d} & ... & x_{nd} \\\\\n",
    "   1 & x_{2d} & x_{2d} & ... & x_{nd} \\\\\n",
    "   1 & ... & ... & ... & ... \\\\\n",
    "   1 & x_{nd} & x_{nd} & ... & x_{nd} \\\\\n",
    "  \\end{array} } \\right] = \\left[ {\\begin{array}{cc}\n",
    "   1 & x_{11} & x_{12}\\\\\n",
    "   1 & x_{21} & x_{22}\\\\\n",
    "   1 & ... & ...\\\\\n",
    "   1 & x_{n1} & x_{n2}\\\\\n",
    "  \\end{array} } \\right] \n",
    "$</center>\n",
    "\n",
    "- Problema con 3 dimensiones y 4 observaciones\n",
    "\n",
    "<center>$\n",
    "  X =\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & x_{1d} & x_{1d} & x_{1d} &... & x_{nd} \\\\\n",
    "   1 & x_{2d} & x_{2d} & x_{2d} &... & x_{nd} \\\\\n",
    "   1 & x_{3d} & x_{3d} & x_{3d} &... & x_{nd} \\\\\n",
    "   1 & x_{4d} & x_{4d} & x_{4d} &... & x_{nd} \\\\\n",
    "  \\end{array} } \\right] = \\left[ {\\begin{array}{cc}\n",
    "   1 & x_{11} & x_{12} & x_{13} \\\\\n",
    "   1 & x_{21} & x_{22} & x_{23} \\\\\n",
    "   1 & x_{31} & x_{32} & x_{33} \\\\\n",
    "   1 & x_{41} & x_{42} & x_{43} \\\\\n",
    "  \\end{array} } \\right] \n",
    "$</center>\n",
    "\n",
    "- Problema con 4 observaciones y 2 dimensiones, modelado con un polinomio de 2º grado\n",
    "\n",
    "<center>$\n",
    "  X =\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & x_{1d} & x_{1d} & x_{1d}^2 &... & x_{nd}^p \\\\\n",
    "   1 & x_{2d} & x_{2d} & x_{2d}^2 &... & x_{nd}^p \\\\\n",
    "   1 & x_{3d} & x_{3d} & x_{3d}^2 &... & x_{nd}^p \\\\\n",
    "   1 & x_{4d} & x_{4d} & x_{4d}^2 &... & x_{nd}^p \\\\\n",
    "  \\end{array} } \\right] = \\left[ {\\begin{array}{cc}\n",
    "   1 & x_{11} & x_{12} & x_{11}^2 & x_{12}^2 \\\\\n",
    "   1 & x_{21} & x_{22} & x_{21}^2 & x_{22}^2 \\\\\n",
    "   1 & x_{31} & x_{32} & x_{31}^2 & x_{32}^2 \\\\\n",
    "   1 & x_{41} & x_{42} & x_{41}^2 & x_{42}^2 \\\\\n",
    "  \\end{array} } \\right] \n",
    "$</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Gradient Descent\n",
    "\n",
    "Hemos visto como **Least Squares/Mínimos Cuadrados** nos permite encontrar la recta que mejor aproxima nuestros datos. Los métodos que nos permiten encontrar la solución mínima o máxima se llaman **métodos de optimización**. Existe numerosos tipos de métodos de optimización, pero nos vamos a centrar en uno de los más usados hoy en día en el mundo del Machine Learning - **Gradient Descent**.\n",
    "\n",
    "**Gradient Descent** es un método de optimización iterativo (cada iteración que ejecutamos estamos un poco más cerca del mínimo de la función - solución). \n",
    " \n",
    "\n",
    "<center>$y_{i} \\approx  f(x_{i}; w) = w_0 + \\sum_{j=1}^d x_{ij}w_{j}$</center>\n",
    "\n",
    "Podemos convertirlo a matrices:\n",
    "\n",
    "<center>$y_{i} \\approx  f(x_{i}; w) = XW$</center>\n",
    "\n",
    "Y queremos encontrar:\n",
    "\n",
    "* **Los argumentos $w$ que minimiza la función** - $\\underset{w}{\\operatorname{argmin}}f(w)$\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Nota</b> $\\min f(x)$ y $\\arg \\min f(x)$ son cosas distintas. Un ejemplo:\n",
    "</div> \n",
    "\n",
    "Considera la función $f(x) = 100 + (x - 3)^2$\n",
    "\n",
    "$\\min f(x) = f(\\arg \\min f(x))$\n",
    "\n",
    "Entonces:\n",
    "\n",
    "$\\frac{df(x)}{x} = 2(x - 3)$<br>\n",
    "$\\arg \\min 2(x - 3) = 0$<br>\n",
    "$\\arg \\min f(x) = 3$\n",
    "\n",
    "Por otro lado:\n",
    "\n",
    "$\\min f(x) = f(\\arg \\min f(x)) = 100 + (3 - 3)^2$<br>\n",
    "$\\min f(x) = 100$\n",
    "\n",
    "Podemos graficarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axs = plt.subplots() \n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "# calculate the y value for each element of the x vector\n",
    "y = 100 + (x - 3)**2 \n",
    "\n",
    "axs.annotate('arg min f(x)',\n",
    "            xy=(0.85, 0), xycoords='axes fraction',\n",
    "            xytext=(-0, 0), textcoords='offset pixels',\n",
    "            horizontalalignment='right',\n",
    "            verticalalignment='bottom',\n",
    "            color=\"r\")\n",
    "\n",
    "axs.annotate('min f(x)',\n",
    "            xy=(0, 0.1), xycoords='axes fraction',\n",
    "            xytext=(-0, 0), textcoords='offset pixels',\n",
    "            horizontalalignment='right',\n",
    "            verticalalignment='bottom',\n",
    "            color=\"r\")\n",
    "\n",
    "axs.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como queremos encontrar el valor de $w$ que hace mínimo $f(x)$, es decir $\\arg \\min f(x)$, los pasos a seguir son:\n",
    "\n",
    "* Empezar en algún punto aleatorio de la curva\n",
    "* Iterativamente, movernos en la dirección de mayor pendiente $\\nabla f(x)$ con pasos con tamaño $\\eta$\n",
    "\n",
    "\\begin{equation}\n",
    "    x := x - \\eta \\cdot \\nabla f(x)\n",
    "\\end{equation}\n",
    "\n",
    "El parámetro $\\eta$ se denomina **learning rate** y es el parámetro que se encarga de medir el \"step size\" en cada iteración mientras nos movemos hacia el mínimo de la función. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>¿Por qué el valor negativo en la ecuación?</b>\n",
    "</div> \n",
    "\n",
    "\n",
    "**DEMO GRADIENT DESCENT:** https://developers.google.com/machine-learning/crash-course/fitter/graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GD(): \n",
    "    def __init__(self, eta=0.1, x0=0, max_iter=50, diff_to_stop=0.01, delta=0.01): \n",
    "        self.x0 = x0 #randomly initialize any value\n",
    "        self.delta = delta #used for gradient calculations\n",
    "        self.eta = eta #learning rate \n",
    "        self.diff_to_stop = diff_to_stop #stop the algorithms if steps are smaller than this value\n",
    "        self.max_iter = max_iter #when to stop\n",
    "\n",
    "    def run(self, f, plot_method=None):       \n",
    "        plt.figure(figsize=(8, 4))\n",
    "        xx = np.linspace(-10, 10, 100)\n",
    "        yy = f(xx)\n",
    "        plt.plot(xx, yy)\n",
    "        plt.grid()\n",
    "        \n",
    "        x_new = self.x0\n",
    "        iter_no = 0\n",
    "        while True:\n",
    "            iter_no += 1\n",
    "\n",
    "            #main algorithm\n",
    "            x = x_new #note: x_new is the position of the previous move\n",
    "            grad = (f(x+self.delta) - f(x))/self.delta #evaluate the gradient at f(x)\n",
    "            x_new = x - self.eta*grad #move in the direction of gradient\n",
    "            #end of main algorithm\n",
    "\n",
    "            if plot_method is 'scatter':\n",
    "                plt.scatter(x_new, f(x_new), color='r')\n",
    "            else:\n",
    "                plt.arrow(x, f(x), x_new-x, f(x_new)-f(x), head_width=0.3, head_length=2, color='r')\n",
    "\n",
    "            step_size = np.abs(x_new - x)\n",
    "            if step_size <= self.diff_to_stop:\n",
    "                print('Successfully converged with a step size of {} after {} iterations!'.format(step_size, iter_no))\n",
    "                plt.scatter(x_new, f(x_new), s=200, marker='*')\n",
    "                break\n",
    "            elif iter_no == self.max_iter:\n",
    "                print('Max iterations completed! Convergence cannot be guaranteed. Step size={}'.format(step_size))\n",
    "                plt.scatter(x_new, f(x_new), s=200, marker='*')\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Vamos a analizar que sucede cuando modificamos los parámetros</b>\n",
    "</div> \n",
    "\n",
    "* Incrementa el número de iteraciones a 50 Increase the maximum number of iterations in order to 50.\n",
    "* Cambia el learning rate a diferentes valores $\\eta \\in \\{0.05, 0.1, 0.8, 1.1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, p=1, q=0, r=0):\n",
    "    return p*(x-q)**2 + r\n",
    "\n",
    "GD(eta=0.00001, x0=8, max_iter=5, diff_to_stop=0.01, delta=0.01).run(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué sucede en este caso? \n",
    "$$f(x) = -\\exp(-(x-2)^2) - 0.5 \\exp(-(x+2)^2)$$\n",
    "\n",
    "Sugerencia: max_iter en $100$, x0 en $-3$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_multimodal(x):\n",
    "    return -np.exp(-(x-2)**2) - 0.5*np.exp(-(x+2)**2)\n",
    "\n",
    "GD(eta=0.1, x0=-3, max_iter=100, diff_to_stop=0.01, delta=0.01).run(f_multimodal, plot_method='scatter')\n",
    "\n",
    "# Local minimum - Alguna solución?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent para Linear Regression\n",
    "\n",
    "Aplicar Gradient Descent para un modelo de regression lineal no tiene ninguna complejidad. Nuestro modelo viene dado por la siguiente ecuación:\n",
    "\n",
    "$$f(x)=w_0+w^T x.$$\n",
    "\n",
    "Debemos optimizar esa función para encontrar los valores de $w$ que la hacen mínima, ¿cómo medimos \"qué la hace mínima y qué no\"? Usando **Residual Sum Squares (RSS)**, que basicamente mide cuanto se aleja el modelo de los valores reales.\n",
    "\n",
    "<img src=\"Images/least_squares.png\" style=\"width: 340px;\"/>\n",
    "\n",
    "**¿Pero esto no es lo mismo que Least Squares?** Si y no... **Least Squares es un método para construir un modelo y RSS es una medida**. Se basan en lo mismo, en medir la distancia entre los valores predichos y los valores verdaderos. Entonces, aplicando RSS a nuestra regresión:\n",
    "\n",
    "$$ RSS(w) = \\sum_{n=1}^{N}[y_n-f(x_n)]^2 =  \\sum_{n=1}^{N}[y_n- (w_0 + \\sum_{d=1}^{D}w_dx_{nd}) ]^2 .$$\n",
    "\n",
    "$$ RSS(w) = \\frac{1}{2}\\sum_{n=1}^{N}[y_n-f(x_n)]^2$$\n",
    "\n",
    "Y lo que queremos es minimizar esta distancia, para que el modelo se acerque lo máximo posible a los valores verdaderos.\n",
    "\n",
    "$$\\nabla RSS(w) = X^T(Xw^t-y)$$\n",
    "\n",
    "En resumen, el gradient descendiente para una regresión lineal, se basa en resolver esta ecuación de forma iterativa:\n",
    "\n",
    "$$w^{t+1} = w^t - \\eta * \\nabla RSS(w)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
