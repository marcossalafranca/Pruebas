{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aperitivo de redes neuronales游땎 - Perceptr칩n\n",
    "\n",
    "Al principio de este tema hemos visto que los problemas de `Machine Learning` se pueden clasificar en base a m칰ltiples cosas y nos hemos centrado principalmente en la categor칤a de `supervised learning` y en concreto hemos visto las `regresiones` (que continuar치n en un ratito). En este **mini-aperitivo de redes neuronales** vamos a ver la importancia de las **funciones lineales** en el 치mbito del ML.\n",
    "\n",
    "Si nos dicen de pensar en un problema de `clasificaci칩n` hay m칰ltiples problemas que se nos pueden venir a la mente cuando hablamos de `problemas de clasificaci칩n`, por ejemplo:\n",
    "\n",
    "- Clasificar si un email es SPAM o NO SPAM\n",
    "- Clasificar si un libro trata sobre fantas칤a, aventura, ...\n",
    "- Clasificar si un objecto en una im치gen es un perro o un gato.\n",
    "\n",
    "Si en el caso de que queremos clasificarlo en solo 2 categ칩rias, es un problema **BINARIO**.\n",
    "\n",
    "- Problema binario: 2 categor칤as\n",
    "- Problema no binario: N categor칤as (Dentro hay m치s clasificaciones que se suelen hacer)\n",
    "\n",
    "### Un momento... esto est치 muy bien pero... 쯗칩nde entran las funciones lineales en juego?\n",
    "\n",
    "Imaginate un problema de `clasificaci칩n binario`, por ejemplo clasificar si un estudiante va a entrar en la universidad en base a su **nota media durante bachiller (Grades)** y en base a su **nota media de selectividad (Test)**. El objetivo es conseguir dibujar una linea en nuestros datos, llamada `boundary line` que nos separe este conjunto de datos:\n",
    "\n",
    "<img src=\"./images/nn_0.png\" width=\"400\">\n",
    "\n",
    "Como ya hemos visto, la ecuaci칩n de una funci칩n lineal (escrita en forma vectorial) viene definida por:<br><br>\n",
    "\n",
    "<center>$\\vec{W}\\vec{x} + b = 0$</center>\n",
    "\n",
    "Escrita para 2 variables $x_1$ y $x_2$:<br>\n",
    "<center>$w_{1}x_{1} + w_{2}x_{2} + b = 0$</center>\n",
    "\n",
    "Esta ecuaci칩n la podemos representar gr치ficamente de una forma muy sencilla que nos llevar치 a entender la unidad b치sica de las redes neuronales, los **perceptrons**\n",
    "\n",
    "<img src=\"./images/nn_01.png\" width=\"200\">\n",
    "\n",
    "El par치metro `b` de nuestra ecuaci칩n, como ya hemos visto, se denomia `bias` y debe tenerse en cuenta a la hora de representar dicha ecuaci칩n de una forma gr치fica, por tanto:\n",
    "\n",
    "<img src=\"./images/nn_02.png\" width=\"200\">\n",
    "\n",
    "Ya hemos conseguido representar gr치ficamente nuestra ecuaci칩n lineal pero las **redes neuronales** necesitan algo que entiendan y en ese caso es una **probabilidad** por lo que a침adiremos un bloque m치s al final de este perceptron, que denominaremos `Funci칩n de Activaci칩n` (que ya ver칠is m치s adelante que significa).\n",
    "\n",
    "<img src=\"./images/nn_03.png\" width=\"300\">\n",
    "\n",
    "Las `funciones de activaci칩n` son muy importantes para que una red neuronal aprenda, su principar objetivo es convertir un input a una se침al de salida, y esta se침al de salida como input para el siguiente nodo, etc... Existen diferentes tipos de funciones de activaci칩n y debemos usar una u otra dependiendo de nuestro problema.\n",
    "\n",
    "Si aplicamos una funci칩n de salida llamada `sigmoid` (no os preocup칠is que la ver칠is en un futuro muchas veces) a la salida de nuestra ecuaci칩n $\\vec{W}\\vec{x} + b$, convertimos el resultado de nuestra ecuaci칩n a un valor entre $0$ y $1$, por ejemplo:\n",
    "\n",
    "$$\\sigma (x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "$$\\vec{W}\\vec{x} + b = 0 \\rightarrow SIGMOID \\rightarrow \\sigma = 0.5$$\n",
    "\n",
    "Por tanto, regraficando la figura anterior obtenemos:\n",
    "<img src=\"./images/nn_04.png\" width=\"300\">\n",
    "\n",
    "Si convertimos esta representaci칩n b치sica a formato matem치tico, teniendo en cuenta que `y_pred` es el valor de nuestra predicci칩n para un input dado, obtenemos:\n",
    "\n",
    "$$\\hat{y} = \\sigma (\\vec{W}\\vec{x} + b)$$\n",
    "\n",
    "Y esta maravillosa ecuaci칩n es la que rige el compartimiento de los `perceptrons`, que es la unidad b치sica para las **redes neuronales.** Si, tu maravillosa red neuronal est치 construida a base de **funciones lineales** que al combinarlas nos permiten modelar nuestro problema como si fuera **no lineal**.\n",
    "\n",
    "<img src=\"./images/funcion_lineal.jpg\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
