{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aperitivo de redes neuronales😃 - Perceptrón\n",
    "\n",
    "Al principio de este tema hemos visto que los problemas de `Machine Learning` se pueden clasificar en base a múltiples cosas y nos hemos centrado principalmente en la categoría de `supervised learning` y en concreto hemos visto las `regresiones` (que continuarán en un ratito). En este **mini-aperitivo de redes neuronales** vamos a ver la importancia de las **funciones lineales** en el ámbito del ML.\n",
    "\n",
    "Si nos dicen de pensar en un problema de `clasificación` hay múltiples problemas que se nos pueden venir a la mente cuando hablamos de `problemas de clasificación`, por ejemplo:\n",
    "\n",
    "- Clasificar si un email es SPAM o NO SPAM\n",
    "- Clasificar si un libro trata sobre fantasía, aventura, ...\n",
    "- Clasificar si un objecto en una imágen es un perro o un gato.\n",
    "\n",
    "Si en el caso de que queremos clasificarlo en solo 2 categórias, es un problema **BINARIO**.\n",
    "\n",
    "- Problema binario: 2 categorías\n",
    "- Problema no binario: N categorías (Dentro hay más clasificaciones que se suelen hacer)\n",
    "\n",
    "### Un momento... esto está muy bien pero... ¿dónde entran las funciones lineales en juego?\n",
    "\n",
    "Imaginate un problema de `clasificación binario`, por ejemplo clasificar si un estudiante va a entrar en la universidad en base a su **nota media durante bachiller (Grades)** y en base a su **nota media de selectividad (Test)**. El objetivo es conseguir dibujar una linea en nuestros datos, llamada `boundary line` que nos separe este conjunto de datos:\n",
    "\n",
    "<img src=\"./images/nn_0.png\" width=\"400\">\n",
    "\n",
    "Como ya hemos visto, la ecuación de una función lineal (escrita en forma vectorial) viene definida por:<br><br>\n",
    "\n",
    "<center>$\\vec{W}\\vec{x} + b = 0$</center>\n",
    "\n",
    "Escrita para 2 variables $x_1$ y $x_2$:<br>\n",
    "<center>$w_{1}x_{1} + w_{2}x_{2} + b = 0$</center>\n",
    "\n",
    "Esta ecuación la podemos representar gráficamente de una forma muy sencilla que nos llevará a entender la unidad básica de las redes neuronales, los **perceptrons**\n",
    "\n",
    "<img src=\"./images/nn_01.png\" width=\"200\">\n",
    "\n",
    "El parámetro `b` de nuestra ecuación, como ya hemos visto, se denomia `bias` y debe tenerse en cuenta a la hora de representar dicha ecuación de una forma gráfica, por tanto:\n",
    "\n",
    "<img src=\"./images/nn_02.png\" width=\"200\">\n",
    "\n",
    "Ya hemos conseguido representar gráficamente nuestra ecuación lineal pero las **redes neuronales** necesitan algo que entiendan y en ese caso es una **probabilidad** por lo que añadiremos un bloque más al final de este perceptron, que denominaremos `Función de Activación` (que ya veréis más adelante que significa).\n",
    "\n",
    "<img src=\"./images/nn_03.png\" width=\"300\">\n",
    "\n",
    "Las `funciones de activación` son muy importantes para que una red neuronal aprenda, su principar objetivo es convertir un input a una señal de salida, y esta señal de salida como input para el siguiente nodo, etc... Existen diferentes tipos de funciones de activación y debemos usar una u otra dependiendo de nuestro problema.\n",
    "\n",
    "Si aplicamos una función de salida llamada `sigmoid` (no os preocupéis que la veréis en un futuro muchas veces) a la salida de nuestra ecuación $\\vec{W}\\vec{x} + b$, convertimos el resultado de nuestra ecuación a un valor entre $0$ y $1$, por ejemplo:\n",
    "\n",
    "$$\\sigma (x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "$$\\vec{W}\\vec{x} + b = 0 \\rightarrow SIGMOID \\rightarrow \\sigma = 0.5$$\n",
    "\n",
    "Por tanto, regraficando la figura anterior obtenemos:\n",
    "<img src=\"./images/nn_04.png\" width=\"300\">\n",
    "\n",
    "Si convertimos esta representación básica a formato matemático, teniendo en cuenta que `y_pred` es el valor de nuestra predicción para un input dado, obtenemos:\n",
    "\n",
    "$$\\hat{y} = \\sigma (\\vec{W}\\vec{x} + b)$$\n",
    "\n",
    "Y esta maravillosa ecuación es la que rige el compartimiento de los `perceptrons`, que es la unidad básica para las **redes neuronales.** Si, tu maravillosa red neuronal está construida a base de **funciones lineales** que al combinarlas nos permiten modelar nuestro problema como si fuera **no lineal**.\n",
    "\n",
    "<img src=\"./images/funcion_lineal.jpg\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
